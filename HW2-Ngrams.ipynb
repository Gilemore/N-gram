{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HW2-Ngrams.ipynb","provenance":[],"collapsed_sections":["x-FxyD9HJGiA","b0fnuHwPJJWe","hQCAcrp_JMP_","5ulvlUYzJRjT"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"x-FxyD9HJGiA","colab_type":"text"},"source":["## General setting (data)"]},{"cell_type":"code","metadata":{"id":"fhPG0olAJjH-","colab_type":"code","outputId":"7747b58a-107f-4c1d-8d4b-3609b84c725a","executionInfo":{"status":"ok","timestamp":1578001619031,"user_tz":300,"elapsed":960,"user":{"displayName":"Haoxue Li","photoUrl":"","userId":"02132541566044588005"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# import\n","import os\n","import json\n","try: \n","  import jsonlines\n","except ImportError:\n","  print(\"Installing the package, RESTART THIS CELL\")\n","  !pip install jsonlines\n","try:\n","  from tqdm import tqdm\n","except ImportError:\n","  print(\"Installing the package, RESTART THIS CELL\")\n","  !pip install tqdm\n","\n","import shutil\n","import numpy as np\n","from collections import defaultdict\n","\n","from google.colab import drive\n","drive.mount(\"/content/drive\")"],"execution_count":61,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jSyYQO2SI_ou","colab_type":"code","colab":{}},"source":["# data\n","def load_wikitext(filename=\"wikitext2-sentencized.json\"):\n","  if not os.path.exists(filename):\n","    !wget \"https://nyu.box.com/shared/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\" -O $filename\n","  \n","  datasets=json.load((open(filename,'r')))\n","  for name in datasets:\n","    datasets[name]=[x.split() for x in datasets[name]]\n","  vocab=list(set([t for ts in datasets[\"train\"] for t in ts]))\n","  print(\"Vocab size: %d\"%(len(vocab)))\n","  return datasets,vocab"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"r-uhSldmM-rl","colab_type":"code","outputId":"c133f8bb-e328-4561-9c87-b14961ffb24c","executionInfo":{"status":"ok","timestamp":1578001621553,"user_tz":300,"elapsed":1502,"user":{"displayName":"Haoxue Li","photoUrl":"","userId":"02132541566044588005"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["datasets,vocab=load_wikitext()"],"execution_count":63,"outputs":[{"output_type":"stream","text":["Vocab size: 33175\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ClrqzNKoNplZ","colab_type":"code","outputId":"4be4d9c9-4cc3-4f18-f67e-53f3d4b03240","executionInfo":{"status":"ok","timestamp":1578001623340,"user_tz":300,"elapsed":1218,"user":{"displayName":"Haoxue Li","photoUrl":"","userId":"02132541566044588005"}},"colab":{"base_uri":"https://localhost:8080/","height":527}},"source":["datasets[\"train\"][0]"],"execution_count":64,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Valkyria',\n"," 'of',\n"," 'the',\n"," 'Battlefield',\n"," '3',\n"," ')',\n"," ',',\n"," 'commonly',\n"," 'referred',\n"," 'to',\n"," 'as',\n"," 'Valkyria',\n"," 'Chronicles',\n"," 'III',\n"," 'outside',\n"," 'Japan',\n"," ',',\n"," 'is',\n"," 'a',\n"," 'tactical',\n"," 'role',\n"," '@-@',\n"," 'playing',\n"," 'video',\n"," 'game',\n"," 'developed',\n"," 'by',\n"," 'Sega',\n"," 'and',\n"," 'Media.']"]},"metadata":{"tags":[]},"execution_count":64}]},{"cell_type":"code","metadata":{"id":"LcrWWTluMozR","colab_type":"code","colab":{}},"source":["def perplexity(model,sequences):\n","  n_total=0\n","  logp_total=0\n","  for sequence in sequences:\n","    logp_total+=model.sequence_logp(sequence)\n","    n_total+=len(sequence)+1\n","  ppl=2**(-(1.0/n_total)*logp_total)\n","  return ppl"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b0fnuHwPJJWe","colab_type":"text"},"source":["## Addictive smoothing"]},{"cell_type":"code","metadata":{"id":"ojtT8cbwJL7Z","colab_type":"code","colab":{}},"source":["class NGramAddictive():\n","  def __init__(self,n,delta,vocab):\n","    self.n=n # n-gram\n","    self.delta=delta # pseudo-count\n","    self.count=defaultdict(lambda: defaultdict(float))\n","    self.total=defaultdict(float)\n","    self.vsize=len(vocab)+1 # +1 is for <eos>, but <bos> will not appear in word \n","\n","  def estimate(self,sequences):\n","    for sequence in sequences:\n","      padded_sequence=[\"<bos>\"]*(self.n-1)+sequence+[\"<eos>\"]\n","      for i in range(len(padded_sequence)-self.n+1):\n","        ngram= tuple(padded_sequence[i:i+self.n])\n","        prefix,word=ngram[:-1],ngram[-1]\n","        self.count[prefix][word]+=1\n","        self.total[prefix]+=1\n","  def sequence_logp(self,sequence):\n","    padded_sequence=[\"<bos>\"]*(self.n-1)+sequence+[\"<eos>\"]\n","    total_logp=0\n","    for i in range(len(padded_sequence)-self.n+1):\n","      ngram=tuple(padded_sequence[i:i+n])\n","      prefix,word=ngram[:-1],ngram[-1]\n","      logp=(self.count[prefix][word]+self.delta)/(self.total[prefix]+self.vsize*self.delta)\n","      total_logp+=np.log2(logp)\n","    return total_logp"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eiPuMO6RPqD8","colab_type":"code","outputId":"13590dcb-39bf-4fb7-a3a7-06ba13921d36","executionInfo":{"status":"ok","timestamp":1577997581782,"user_tz":300,"elapsed":33270,"user":{"displayName":"Haoxue Li","photoUrl":"","userId":"02132541566044588005"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["delta=0.0005\n","for n in [2,3,4]:\n","  lm=NGramAddictive(n,delta,vocab)\n","  lm.estimate(datasets[\"train\"])\n","  print(\"Baseline (Additive smoothing, n=%d, delta=%.4f)) Train Perplexity: %3.f\"% (n,delta,perplexity(lm,datasets[\"train\"])))\n","  print(\"Baseline (Additive smoothing, n=%d, delta=%.4f)) Valid Perplexity: %.3f\" % (n, delta, perplexity(lm, datasets['valid'])))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Baseline (Additive smoothing, n=2, delta=0.0005)) Train Perplexity:  90\n","Baseline (Additive smoothing, n=2, delta=0.0005)) Valid Perplexity: 525.825\n","Baseline (Additive smoothing, n=3, delta=0.0005)) Train Perplexity:  27\n","Baseline (Additive smoothing, n=3, delta=0.0005)) Valid Perplexity: 2577.128\n","Baseline (Additive smoothing, n=4, delta=0.0005)) Train Perplexity:  20\n","Baseline (Additive smoothing, n=4, delta=0.0005)) Valid Perplexity: 9570.901\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hQCAcrp_JMP_","colab_type":"text"},"source":["## Interpolation"]},{"cell_type":"code","metadata":{"id":"TBRsUwQeJRIZ","colab_type":"code","colab":{}},"source":[" class NGramInterpolation:\n","    def __init__(self,n,vocab,la):\n","     self.n=n\n","     self.vsize=len(vocab)+1\n","     self.la=la\n","     self.count1=defaultdict(float)\n","     self.count=defaultdict(lambda: defaultdict(lambda: defaultdict(float)))\n","     self.total1=0\n","     self.total=defaultdict(lambda: defaultdict(float))\n","\n","    def estimate(self,sequences):\n","      for j in range(1,self.n+1):\n","        for sequence in sequences:\n","          padded_sequence=[\"<bos>\"]*(j-1)+sequence+[\"<eos>\"]\n","          for i in range(len(padded_sequence)-j+1):\n","            ngram=tuple(padded_sequence[i:i+j])\n","            #if j==2:\n","             # print(j,ngram,i,len(padded_sequence))\n","            prefix,word=ngram[:-1],ngram[-1]\n","            if j==1: \n","              self.count1[word]+=1\n","              self.total1+=1\n","            else:\n","              self.count[j][tuple(prefix)][word]+=1\n","              self.total[j][tuple(prefix)]+=1\n","    def sequence_logp(self,sequence):\n","      total_logp=0\n","      padded_sequence=[\"<bos>\"]*(self.n-1)+sequence+[\"<eos>\"]\n","      for i in range(len(padded_sequence) - self.n+1):\n","            ngram = tuple(padded_sequence[i:i+self.n])\n","            total_logp += np.log2(self.ngram_prob(ngram))\n","      return total_logp\n","  \n","    def ngram_prob(self,ngram):\n","      # p=la1*p(word|prefix)+la2*p(word|prefix[1:])\n","      p=(1/self.vsize)*self.la[0]\n","      for i in range(1,self.n+1): # i=2\n","        prefix,word=ngram[-i:-1],ngram[-1] # prefix=ngram[2,3]\n","        if i==1:\n","          p+=self.la[i]*self.count1[word]/self.total1\n","        else:\n","          p+=self.la[i]*self.count[i][tuple(prefix)][word]/max(self.total[i][tuple(prefix)],1) # p+=self.la[2]*self.count[2][]\n","      return p"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KQEEBMgxo5kA","colab_type":"code","colab":{}},"source":["def show_results(l,n):\n","  for la in l:\n","    print(\"\\nlambda values are: \",la)\n","    lm=NGramInterpolation(n,vocab,la)\n","    lm.estimate(datasets[\"train\"])\n","\n","    print(\"Baseline (Interpolation, n=%d)) Train Perplexity: %.3f\" % (n,  perplexity(lm, datasets['train'])))\n","    print(\"Baseline (Interpolation, n=%d)) Valid Perplexity: %.3f\" % (n,  perplexity(lm, datasets['valid'])))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EmCmLBKhlNDX","colab_type":"code","outputId":"f65a4e42-21a0-447f-b3f7-ff3a22a9acd0","executionInfo":{"status":"ok","timestamp":1577925729187,"user_tz":300,"elapsed":191810,"user":{"displayName":"Haoxue Li","photoUrl":"","userId":"02132541566044588005"}},"colab":{"base_uri":"https://localhost:8080/","height":649}},"source":["la2=[[1/3,1/3,1/3],\n","     [1/6,1/3,1/2],\n","     [1/6,1/2,1/3]]\n","la3=[[1/4,1/4,1/4,1/4],\n","     [1/10,1/5,3/10,2/5],\n","     [1/10,1/5,4/10,3/10]]\n","la4=[[1/5,1/5,1/5,1/5,1/5],\n","     [1/15,2/15,3/15,4/15,5/15],\n","     [1/15,2/15,5/15,4/15,3/15]]\n","show_results(la2,2)\n","show_results(la3,3)\n","show_results(la4,4)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","lambda values are:  [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]\n","Baseline (Interpolation, n=2)) Train Perplexity: 179.054\n","Baseline (Interpolation, n=2)) Valid Perplexity: 395.603\n","\n","lambda values are:  [0.16666666666666666, 0.3333333333333333, 0.5]\n","Baseline (Interpolation, n=2)) Train Perplexity: 129.816\n","Baseline (Interpolation, n=2)) Valid Perplexity: 326.710\n","\n","lambda values are:  [0.16666666666666666, 0.5, 0.3333333333333333]\n","Baseline (Interpolation, n=2)) Train Perplexity: 170.081\n","Baseline (Interpolation, n=2)) Valid Perplexity: 368.773\n","\n","lambda values are:  [0.25, 0.25, 0.25, 0.25]\n","Baseline (Interpolation, n=3)) Train Perplexity: 23.326\n","Baseline (Interpolation, n=3)) Valid Perplexity: 326.292\n","\n","lambda values are:  [0.1, 0.2, 0.3, 0.4]\n","Baseline (Interpolation, n=3)) Train Perplexity: 15.778\n","Baseline (Interpolation, n=3)) Valid Perplexity: 306.898\n","\n","lambda values are:  [0.1, 0.2, 0.4, 0.3]\n","Baseline (Interpolation, n=3)) Train Perplexity: 18.541\n","Baseline (Interpolation, n=3)) Valid Perplexity: 290.589\n","\n","lambda values are:  [0.2, 0.2, 0.2, 0.2, 0.2]\n","Baseline (Interpolation, n=4)) Train Perplexity: 7.017\n","Baseline (Interpolation, n=4)) Valid Perplexity: 358.526\n","\n","lambda values are:  [0.06666666666666667, 0.13333333333333333, 0.2, 0.26666666666666666, 0.3333333333333333]\n","Baseline (Interpolation, n=4)) Train Perplexity: 4.698\n","Baseline (Interpolation, n=4)) Valid Perplexity: 392.056\n","\n","lambda values are:  [0.06666666666666667, 0.13333333333333333, 0.3333333333333333, 0.26666666666666666, 0.2]\n","Baseline (Interpolation, n=4)) Train Perplexity: 6.150\n","Baseline (Interpolation, n=4)) Valid Perplexity: 330.543\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5ulvlUYzJRjT","colab_type":"text"},"source":["## Kenlm"]},{"cell_type":"code","metadata":{"id":"oTaIMpocxmKe","colab_type":"code","colab":{}},"source":["from collections import Counter"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rR-7pJ2BJTpr","colab_type":"code","colab":{}},"source":["class NGramkenlm:\n","  def __init__(self,n,vocab):\n","    self.n=n\n","    self.n_k=[0]*6\n","    self.D=[0]*5\n","    self.count=defaultdict(lambda: defaultdict(lambda: defaultdict(float)))\n","    self.total=defaultdict(lambda: defaultdict(float))\n","    self.count1=defaultdict(float)\n","    self.total1=len(vocab)+1\n","    self.vsize=len(vocab)+1\n","  def estimate(self,sequences):\n","    for i in range(1,self.n+1):\n","      for sequence in sequences:\n","        padded_sequence=[\"<bos>\"]*(i-1)+sequence+[\"<eos>\"]\n","        for j in range(len(padded_sequence)-i+1):\n","          ngram=tuple(padded_sequence[j:i+j])\n","          prefix,word=ngram[:-1],ngram[-1]\n","          if i==1:\n","            self.count1[word]+=1\n","            self.total1+=1\n","          else:\n","            self.count[i][tuple(prefix)][word]+=1\n","            self.total[i][tuple(prefix)]+=1\n","    for t in range(1,5):\n","      for k,v in zip(self.total[self.n].keys(),self.total[self.n].values()):\n","        if int(v)==t:\n","          self.n_k[t]+=1\n","    self.Y=self.n_k[1]/(self.n_k[1]+2*self.n_k[2])\n","    for t in range(1,4):\n","      self.D[t]=t-self.Y*(t+1)*(self.n_k[t+1]/self.n_k[t]) \n","    print(self.D)\n","\n","  def sequence_logp(self,sequence):\n","    total_logp=0\n","    padded_sequence=[\"<bos>\"]*(self.n-1)+sequence+[\"<eos>\"]\n","    for i in range(len(padded_sequence)-self.n+1):\n","      ngram=tuple(padded_sequence[i:i+self.n])\n","      total_logp+=np.log2(self.ngram_p(ngram))\n","    return total_logp\n","  def ngram_p(self,ngram):\n","    for i in range(1,self.n+1):\n","      prefix,word=ngram[-i:-1],ngram[-1]\n","      if i==1:\n","        prob=self.count1[word]/self.total1\n","        if self.count1[word]==0:\n","          prob=1/(self.vsize)\n","      else:\n","        prob=self.update_p(i,prefix,word,prob)\n","    return prob\n","\n","  def update_p(self,i,prefix,word,prob):\n","    c=self.count[i][tuple(prefix)][word]\n","    t=self.total[i][tuple(prefix)]\n","    N=[0]*4\n","    if c>0:\n","      prob=(c-self.D[int(c) if c<3 else 3])/max(t,1)\n","      return prob\n","    else:\n","      for k,v in zip(self.count[i][tuple(prefix)].keys(),self.count[i][tuple(prefix)].values()):\n","        N[int(v) if v<3 else 3]+=1\n","      sigma=(self.D[1]*N[1]+self.D[2]*N[2]+self.D[3]*N[3])/max(t,1)\n","      if sigma>0:\n","        prob=sigma*prob\n","      return prob"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PmUwXx6DwU6G","colab_type":"code","outputId":"4fd6ce50-1980-4efb-fc81-d5867b1c0f31","executionInfo":{"status":"ok","timestamp":1578002376779,"user_tz":300,"elapsed":149046,"user":{"displayName":"Haoxue Li","photoUrl":"","userId":"02132541566044588005"}},"colab":{"base_uri":"https://localhost:8080/","height":170}},"source":["for n in [2,3,4]:\n","    lm2 = NGramkenlm(n,vocab)  # +1 is for <eos>\n","    lm2.estimate(datasets['train'])\n","\n","    # print(\"The value of lambda: {}\".format(l[n]))\n","    print(\"Baseline (Interpolation, n=%d)) Train Perplexity: %.3f\" % (n,  perplexity(lm2, datasets['train'])))\n","    print(\"Baseline (Interpolation, n=%d)) Valid Perplexity: %.3f\" % (n,  perplexity(lm2, datasets['valid'])))"],"execution_count":76,"outputs":[{"output_type":"stream","text":["[0, 0.10222222222222221, 0.41008100810080994, 2.7384814814814815, 0]\n","Baseline (Interpolation, n=2)) Train Perplexity: 105.160\n","Baseline (Interpolation, n=2)) Valid Perplexity: 346.055\n","[0, 0.7030687588555738, 1.1273862307842455, 1.4884132492931772, 0]\n","Baseline (Interpolation, n=3)) Train Perplexity: 18.874\n","Baseline (Interpolation, n=3)) Valid Perplexity: 311.165\n","[0, 0.8391219675767698, 1.2510035732353122, 1.4337382971822832, 0]\n","Baseline (Interpolation, n=4)) Train Perplexity: 11.262\n","Baseline (Interpolation, n=4)) Valid Perplexity: 341.428\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2ynGqgTIHfJr","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}